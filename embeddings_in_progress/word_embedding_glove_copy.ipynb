{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP tools\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "# Data tools\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Google vectors\n",
    "! mkdir -p ~/downloads\n",
    "! wget -nc -P ~/downloads http://nlp.stanford.edu/data/glove.6B.zip\n",
    "! unzip -d ~/downloads/glove -o ~/downloads/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove_file = datapath('/home/ubuntu/downloads/glove/glove.6B.50d.txt')\n",
    "tmp_file = get_tmpfile(\"glove_word2vec.txt\")\n",
    "\n",
    "# call glove2word2vec script\n",
    "# default way (through CLI): python -m gensim.scripts.glove2word2vec --input <glove_file> --output <w2v_file>\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take about a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Vectors\n",
    "len(model.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the Vectors\n",
    "model.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "**Exploring Word2Vec Vectors**\n",
    "* Print out a few word vectors from the Google set\n",
    "* Print out the similarity between the following pairs (feel free to experiment with more if you like):\n",
    "  * baseball, bat\n",
    "  * baseball, ocean\n",
    "  * bat, fly\n",
    "* What sorts of patterns do you notice?  Where does it succeed?  Where does it fail?  How might one improve it?\n",
    "* Print out the most similar words to the following words:\n",
    "  * baseball\n",
    "  * president\n",
    "* Print out words similar to the positive words and dissimilar to the negative words for the following positive/negative groups:\n",
    "* Print out the words that don't match the others in each of the following groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Vectors\n",
    "model.word_vec('baseball')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise Similarity\n",
    "model.similarity('obama', 'clinton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity('obama', 'reagan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most similar words\n",
    "model.similar_by_word('obama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive Negative Similar Words\n",
    "model.most_similar(positive=['obama', 'clinton'], negative=['president'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['king', 'woman'], negative=['man'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"brooklyn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words that don't match\n",
    "model.doesnt_match(['breakfast', 'lunch', 'dinner', 'baseball'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "\n",
    "word similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing via Cosine Similarity\n",
    "model.n_similarity(['obama', 'president'], ['clinton', 'president'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing via Word Mover's Distance\n",
    "model.wmdistance(['obama', 'president'], ['clinton', 'president'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing via Word Mover's Distance\n",
    "model.wmdistance(['obama', 'president'], ['huckabee', 'president'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher because Ted Cruz was not president."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Features\n",
    "\n",
    "Each word contains an array of 300 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.word_vec('cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.word_vec('cat')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity between words can be computed and produces intuitive trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.similarity('cat', 'cat'))\n",
    "print(model.similarity('cat', 'dog'))\n",
    "print(model.similarity('cat', 'car'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.similarity('car', 'truck'))\n",
    "print(model.similarity('car', 'drive'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec captures some interesting similarities between words, such as the relationship between **man --> king** and **woman --> queen**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paris is to France as London is to England\n",
    "# [positive] is to [negative] as [query] is to [positive]\n",
    "\n",
    "model.most_similar(positive=['paris', 'australia'], negative=['france'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stripes are to zebras as spots are to leopards\n",
    "# [positive] is to [negative] as [query] is to [positive]\n",
    "\n",
    "model.most_similar(positive=['spots', 'zebras'], negative=['leopards'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: Why does the previous command take so much longer than others?\n",
    "Because it has to generate a new vector that is **woman** + **king** - **man** and compare that vector to all 3 million vectors, then sort to find the closest 3.  The 3 million are stored in such a way that they can be compared quickly, but any new vector is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wmdistance(\"Obama is the president of the United States\".lower().split(), \n",
    "                        \"Bush was the president of the United States\".lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also detect words that don't belong in a sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
